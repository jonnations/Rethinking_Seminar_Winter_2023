---
title: "Multicollinearity"
author: "Jon Nations"
format:
  html:
    toc: true
    toc-depth: 5
    #html-math-method: katex
#editor: visual
date: last-modified
---

Multicollinearity is important, so let's talk about it.

```{r}
#| echo: false
pacman::p_load(tidyverse, brms, cmdstanr, tidybayes, patchwork)
```

But first, I want to plot the graph from the opening of Chapter 6 on selection bias in science. As mentioned in the text, this is a real problem. I'm sure you can think of examples in your field of papers or funded projects that fall into these categories. 

### Selection Bias

```{r}
#| echo: false
set.seed(1914)
n <- 200  # number of grant proposals
p <- 0.1  # proportion to select

d <-
  # uncorrelated newsworthiness and trustworthiness
  tibble(newsworthiness  = rnorm(n, mean = 0, sd = 1),
         trustworthiness = rnorm(n, mean = 0, sd = 1)) %>% 
  # total_score
  mutate(total_score = newsworthiness + trustworthiness) %>% 
  # select top 10% of combined scores
  mutate(selected = ifelse(total_score >= quantile(total_score, 1 - p), TRUE, FALSE))
```

```{r}
#| warning: false
#| echo: false
#| fig.width: 6
#| fig.height: 5.75
text <-
  tibble(newsworthiness  = c(2, 1), 
         trustworthiness = c(2.25, -2.5),
         selected        = c(TRUE, FALSE),
         label           = c("selected", "rejected"))

d %>% 
  ggplot(aes(x = newsworthiness, y = trustworthiness, color = selected)) +
  geom_point(aes(shape = selected), alpha = 3/4) +
  geom_text(data = text,
            aes(label = label),
            size = 5) +
  geom_smooth(data = . %>% filter(selected == TRUE),
              method = "lm", fullrange = T,
              color = "firebrick4", se = F, size = 1/2) +
  scale_color_manual(values = c("black", "firebrick4")) +
  scale_shape_manual(values = c(1, 19)) +
  scale_x_continuous(limits = c(-3, 3.9), expand = c(0, 0)) +
  coord_cartesian(ylim = range(d$trustworthiness)) +
  theme_bw() +
  theme(legend.position = "none",
        axis.title = element_text(size=14,face="bold")) + coord_fixed() +
  labs(title = "Selection-Distortion Effect",
       subtitle = "Sadly, this is real. Something to think about...")
```

## Multicollinearity: Simulated Example

Multicollinearity is historically a big topic of conversation in applied statistics. Now that you've heard about it, you will start to see it pop up in a lot of places. Often, you will see it spoken of as this evil feature of data, the worst thing possible, and a reason why your analyses may be totally flawed!!

I think this book does a good job explaining what it is, and why it happens, and how to look out for it. It's not some evil demon lurking, rather, it's just a feature of regression. And, in this case, it results in a false negative rather than a false positive. 

Simulate some data
```{r}
#| warning: false
#| 
#packages used in this example. Check out `pacman` if you haven't already!
pacman::p_load(tidyverse, brms, cmdstanr, tidybayes, patchwork)

n <- 100
set.seed(909)

d <- 
  tibble(
    # height of person
    height   = rnorm(n, mean = 10, sd = 2),
    # proportion of leg to height, note tiny interval
    leg_prop = runif(n, min = 0.4, max = 0.5)) %>% 
  #calculate left leg values and right leg values
  mutate(leg_left  = leg_prop * height + rnorm(n, mean = 0, sd = 0.02),
         leg_right = leg_prop * height + rnorm(n, mean = 0, sd = 0.02))

d
```
What happens when we use these two highly correlated variables in our analysis?

$$
\begin{align*}
\text{Height}_i   & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i & = \alpha + \beta_1 \text{Left-Leg}_i + \beta_2 \text{Right-Leg}_i.
\end{align*}
$$


```{r}
#| warning: false
#| echo: false
#| cache: true
#| include: false

b6.1 <- 
  brm(data = d, 
      family = gaussian,
      height ~ 1 + leg_left + leg_right,
      prior = c(prior(normal(10, 100), class = Intercept),
                prior(normal(2, 10), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 6,
      refresh = 0,
      backend = "cmdstan")
```



```{r}
#| warning: false
#| echo: false
b6.1 %>% 
  gather_draws(b_leg_left, b_leg_right, sigma, b_Intercept) %>% 
  ggplot(aes(x = .value, y = .variable)) +
  stat_pointinterval(.width = c(.66, .89), color = "forestgreen", fill = "forestgreen") +
  labs(x = "posterior", y = NULL,
       title = "The coefficient plot for the two-leg model",
       subtitle = "These are REALLY BIG Intervals! Something seems wrong") +
  xlim(-4,6) +
  theme(axis.text.y = element_text(hjust = 0),
        panel.border = element_rect(color = "black", fill = "transparent"),
        panel.grid.minor = element_blank(),
        strip.text = element_text(hjust = 0)) +
  theme_bw()
  
```

Why does this happen?
```{r}
#| warning: false
#| echo: false
#| cache: true

post <- as_draws_df(b6.1)
  
post %>% 
  ggplot(aes(x = b_leg_left, y = b_leg_right)) +
  geom_point(color = "forestgreen", alpha = 1/10, size = 1/2) + theme_bw() +
  labs(title = "Variables are Highly Correlated!",
       subtitle = "When Left leg is a large value, Right Leg is a small value") + coord_fixed()
```
What can't be seen from this plot however is that the correlation is related by draw. When the model estimates a high value for right leg, it estimates an almost equally low value for right leg, with the mean value as the center point!

See if you can spot that here:
```{r}
#| warning: false
post %>% 
  select(b_Intercept, b_leg_left, b_leg_right) %>% 
  # calculate the mean value of the left and right leg estimates
  mutate(mean_leg = (b_leg_left + b_leg_right)/2)
```
Look at how those mean values are similar!!!

Interestingly, if we simply sum the beta values together...

```{r fig.width = 4, fig.height = 3, warning = F, message = F, echo = F}

p1 <- post %>% 
  ggplot(aes(x = b_leg_left + b_leg_right, y = 0)) +
 stat_slabinterval(quantiles = 200, point_interval = median_qi, 
               fill = "forestgreen", color = "black", .width = .89, alpha = 0.8) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Sum of the beta values of the\nmulticollinear coefficients",
       subtitle = "Marked by the median and 89% PIs") + 
  xlim(1.7, 2.3) + theme_bw() 
p1
```

This is weird - What happens in a model with just one leg?

```{=tex}
\begin{align*}
\text{Height}_i   & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i & = \alpha + \beta_1 \text{Left-Leg}_i
\end{align*}
```

```{r}
#| warning: false
#| echo: false
#| cache: true
#| include: false

b6.2 <- 
  brm(data = d, 
      family = gaussian,
      height ~ 1 + leg_left ,
      prior = c(prior(normal(10, 100), class = Intercept),
                prior(normal(2, 10), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 6,
      refresh = 0,
      backend = "cmdstan")

post2 <- as_draws_df(b6.2)

p2 <- post2 %>% 
  ggplot(aes(x = b_leg_left, y = 0)) +
 stat_slabinterval(quantiles = 200, point_interval = median_qi, 
               fill = "forestgreen", color = "black", .width = .89, alpha = 0.8) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlim(1.7, 2.3) +
  labs(title = "Model with Only One Leg",
       subtitle = "Marked by the median and 89% PIs") + theme_bw()
```


```{r fig.width = 8.5, fig.height = 3, warning = F, message = F}
 #patchwork plotting - the best figure-making package ever!
p1 + p2
```



> The basic lesson is only this: When two predictor variables are very strongly correlated (conditional on other variables in the model), including both in a model may lead to confusion. The posterior distribution isn't wrong, in such cases. It's telling you that the question you asked cannot be answered with these data. And that's a great thing for a model to say, that it cannot answer your question. And if you are just interested in prediction, you'll find that this leg model makes fine predictions. It just doesnâ€™t make any claims about which leg is more important. (p. 166)

How do we deal with multicollinearity?  

1. Explore your data!  

2. Think about a **Scientific Model**, preferably using causal reasoning and Directed Acyclic Graphs, to see the effects of one variable on the other, so you know what to "control for."  
3. There are various methods of variable selection which can identify co-linear variables. Variance Inflation Factor (VIF) is an older but still commonly used method. Lasso or horseshoe priors are another interesting way to do variable selection. These are really tight priors with really long narrow tails. The idea is that any variable with a large effect size has enough influence to end up in posterior space far away from the mean. A newer method that works with modern Bayesian Multilevel Modeling is Projection Prediction, or [ProjPred](https://mc-stan.org/projpred/), which uses cross validation and model accuracy to determine the importance of different variables. However, these are often easy ways to bypass thoughtful consideration of variables and use "causal salad" to throw it at the wall and see what sticks. ***Nothing can take the place of a well thought out scientific model and causal reasoning that determines the causal relationships between variables!***